{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19dd088f-9bc8-4f7a-abc6-ca45c5dbb1a2",
   "metadata": {},
   "source": [
    "# Systematic Literature Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ece2c-3b10-4aab-95c0-b02047025c81",
   "metadata": {},
   "source": [
    "**Step 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "512e26f8-07eb-4500-b2e3-69bc4280dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Identify columns and match them so combining into 'records.csv' is seamless.\n",
    "\n",
    "pm_cols = {'PMID': 'id',\n",
    " 'DOI': 'doi',\n",
    " 'Authors': 'authors',\n",
    " 'Title': 'title',\n",
    " 'Publication Year': 'year',\n",
    " 'Journal/Book': 'journal'}\n",
    " \n",
    "em_cols = {'Embase Link': 'id',\n",
    " 'DOI':'doi',\n",
    " 'Author Names': 'authors',\n",
    " 'Title': 'title',\n",
    " 'Publication Year': 'year',\n",
    " 'Source': 'journal'}\n",
    " \n",
    "wos_cols = {'UT (Unique WOS ID)':'id', \n",
    " 'DOI':'doi',\n",
    " 'Authors': 'authors',\n",
    " 'Article Title':  'title',\n",
    " 'Publication Year':  'year',\n",
    " 'Source Title': 'journal'}\n",
    "\n",
    "pm_bptb = pd.read_csv('./rct/pubmed/pm_bptb.csv', encoding = 'utf-8', usecols = ['PMID', 'DOI', 'Authors', 'Title', 'Publication Year', 'Journal/Book']).rename(columns = pm_cols)\n",
    "pm_ht = pd.read_csv('./rct/pubmed/pm_ht.csv', encoding = 'utf-8', usecols = ['PMID', 'DOI', 'Authors', 'Title', 'Publication Year', 'Journal/Book']).rename(columns = pm_cols)\n",
    "pm_qt = pd.read_csv('./rct/pubmed/pm_qt.csv', encoding = 'utf-8', usecols = ['PMID', 'DOI', 'Authors', 'Title', 'Publication Year', 'Journal/Book']).rename(columns = pm_cols)\n",
    "pm_plt = pd.read_csv('./rct/pubmed/pm_plt.csv', encoding = 'utf-8', usecols = ['PMID', 'DOI', 'Authors', 'Title', 'Publication Year', 'Journal/Book']).rename(columns = pm_cols)\n",
    "pm_at = pd.read_csv('./rct/pubmed/pm_at.csv', encoding = 'utf-8', usecols = ['PMID', 'DOI', 'Authors', 'Title', 'Publication Year', 'Journal/Book']).rename(columns = pm_cols)\n",
    "pm_ta = pd.read_csv('./rct/pubmed/pm_ta.csv', encoding = 'utf-8', usecols = ['PMID', 'DOI', 'Authors', 'Title', 'Publication Year', 'Journal/Book']).rename(columns = pm_cols)\n",
    "pubmed = pd.concat([pm_bptb, pm_ht, pm_qt, pm_plt, pm_at, pm_ta])\n",
    "pubmed['source'] = 'PubMed/MEDLINE'\n",
    "pubmed.to_csv('./rct/pubmed/pubmed.csv', encoding = 'utf-8')\n",
    "\n",
    "\n",
    "em_bptb = pd.read_csv('./rct/embase/em_bptb.csv', encoding = 'utf-8', usecols =['Embase Link', 'DOI', 'Author Names', 'Title', 'Publication Year', 'Source']).rename(columns = em_cols)\n",
    "em_ht = pd.read_csv('./rct/embase/em_ht.csv', encoding = 'utf-8', usecols = ['Embase Link', 'DOI', 'Author Names', 'Title', 'Publication Year', 'Source']).rename(columns = em_cols)\n",
    "em_qt = pd.read_csv('./rct/embase/em_qt.csv', encoding = 'utf-8', usecols = ['Embase Link', 'DOI', 'Author Names', 'Title', 'Publication Year', 'Source']).rename(columns = em_cols)\n",
    "em_plt = pd.read_csv('./rct/embase/em_plt.csv', encoding = 'utf-8', usecols = ['Embase Link', 'DOI', 'Author Names', 'Title', 'Publication Year', 'Source']).rename(columns = em_cols)\n",
    "em_at = pd.read_csv('./rct/embase/em_at.csv', encoding = 'utf-8', usecols = ['Embase Link', 'DOI', 'Author Names', 'Title', 'Publication Year', 'Source']).rename(columns = em_cols)\n",
    "em_ta = pd.read_csv('./rct/embase/em_ta.csv', encoding = 'utf-8', usecols = ['Embase Link', 'DOI', 'Author Names', 'Title', 'Publication Year', 'Source']).rename(columns = em_cols)\n",
    "embase = pd.concat([em_bptb, em_ht, em_qt, em_plt, em_at, em_ta])\n",
    "embase['source'] = 'Embase'\n",
    "embase.to_csv('./rct/embase/embase.csv', encoding = 'utf-8')\n",
    "\n",
    "wos_bptb = pd.read_csv('./rct/wos/wos_bptb.csv', encoding = 'latin-1', usecols = ['UT (Unique WOS ID)', 'DOI', 'Authors', 'Article Title', 'Publication Year', 'Source Title']).rename(columns = wos_cols)\n",
    "wos_ht = pd.read_csv('./rct/wos/wos_ht.csv', encoding = 'latin-1', usecols = ['UT (Unique WOS ID)', 'DOI', 'Authors', 'Article Title', 'Publication Year', 'Source Title']).rename(columns = wos_cols)\n",
    "wos_qt = pd.read_csv('./rct/wos/wos_qt.csv', encoding = 'latin-1', usecols = ['UT (Unique WOS ID)', 'DOI', 'Authors', 'Article Title', 'Publication Year', 'Source Title']).rename(columns = wos_cols)\n",
    "wos_plt = pd.read_csv('./rct/wos/wos_plt.csv', encoding = 'latin-1', usecols = ['UT (Unique WOS ID)', 'DOI', 'Authors', 'Article Title', 'Publication Year', 'Source Title']).rename(columns = wos_cols)\n",
    "wos = pd.concat([wos_bptb, wos_ht, wos_qt, wos_plt])\n",
    "wos['source'] = 'Web of Science'\n",
    "wos.to_csv('./rct/embase/wos.csv', encoding = 'latin-1')\n",
    "\n",
    "#sco_bptb = pd.read_csv('./rct/scopus/sco_bptb.csv', encoding = 'utf-8')\n",
    "#sco_ht = pd.read_csv('./rct/scopus/sco_ht.csv', encoding = 'utf-8')\n",
    "#sco_qt = pd.read_csv('./rct/scopus/sco_qt.csv', encoding = 'utf-8')\n",
    "#sco_plt = pd.read_csv('./rct/scopus/sco_plt.csv', encoding = 'utf-8')\n",
    "#sco_at = pd.read_csv('./rct/scopus/sco_at.csv', encoding = 'utf-8')\n",
    "#sco_ta = pd.read_csv('./rct/scopus/sco_ta.csv', encoding = 'utf-8')\n",
    "#scopus = pd.concat([sco_bptb, sco_ht, sco_qt, sco_plt, sco_at, sco_ta])\n",
    "#scopus.to_csv('./rct/scopus/scopus.csv', encoding = 'utf-8')\n",
    "\n",
    "# w/ Scopus\n",
    "# doi = pd.concat([pubmed['DOI'], embase['DOI'], wos['DOI'], scopus['DOI']])\n",
    "# authors = pd.concat([pubmed['Authors'], embase['Author Names'], wos['Authors'], scopus['Authors']])\n",
    "# title = pd.concat([pubmed['Title'], embase['Title'], wos['Article Title'], scopus['Title']])\n",
    "# year = pd.concat([pubmed['Publication Year'], embase['Publication Year'], wos['Publication Year'], scopus['Year']])\n",
    "# journal = pd.concat([pubmed['Journal/Book'], embase['Source'], wos['Source Title'], scopus['Source title']])\n",
    "\n",
    "records = pd.concat([pubmed, embase, wos])\n",
    "records['first_author'] = records['authors'].str.replace(r'[,.;]','', regex = True).str.split().str[0]\n",
    "records['short_title'] = records['title'].str.replace(r'[\\[\\]\\s,.;-]','',regex = True).str.lower()\n",
    "records['key'] = records['first_author'] + '+' + records['short_title'] + '+' + records['year'].astype(str)\n",
    "\n",
    "patellar = pd.concat([pm_bptb, em_bptb, wos_bptb])\n",
    "hamstring = pd.concat([pm_ht, em_ht, wos_ht])\n",
    "quadriceps = pd.concat([pm_qt, em_qt, wos_qt])\n",
    "peroneus_longus = pd.concat([pm_plt, em_plt, wos_plt])\n",
    "achilles = pd.concat([pm_at, em_at])\n",
    "tibialis_anterior = pd.concat([pm_ta, em_ta])\n",
    "\n",
    "patellar.to_csv('./by_graft_type/patellar.csv')\n",
    "hamstring.to_csv('./by_graft_type/hamstring.csv')\n",
    "quadriceps.to_csv('./by_graft_type/quadriceps.csv')\n",
    "peroneus_longus.to_csv('./by_graft_type/peroneus_longus.csv')\n",
    "achilles.to_csv('./by_graft_type/achilles.csv')\n",
    "tibialis_anterior.to_csv('./by_graft_type/tibialis_anterior.csv')\n",
    "\n",
    "records.to_csv('./records.csv')\n",
    "records.head()\n",
    "\n",
    "rev_pm_bptb = pd.read_csv('./reviews/bptb.csv', encoding = 'utf-8', usecols = ['PMID', 'DOI', 'Authors', 'Title', 'Publication Year', 'Journal/Book']).rename(columns = pm_cols)\n",
    "rev_pm_ht = pd.read_csv('./reviews/ht.csv', encoding = 'utf-8', usecols = ['PMID', 'DOI', 'Authors', 'Title', 'Publication Year', 'Journal/Book']).rename(columns = pm_cols)\n",
    "rev_pm_qt = pd.read_csv('./reviews/qt.csv', encoding = 'utf-8', usecols = ['PMID', 'DOI', 'Authors', 'Title', 'Publication Year', 'Journal/Book']).rename(columns = pm_cols)\n",
    "rev_pm_plt = pd.read_csv('./reviews/plt.csv', encoding = 'utf-8', usecols = ['PMID', 'DOI', 'Authors', 'Title', 'Publication Year', 'Journal/Book']).rename(columns = pm_cols)\n",
    "rev_pm_at = pd.read_csv('./reviews/at.csv', encoding = 'utf-8', usecols = ['PMID', 'DOI', 'Authors', 'Title', 'Publication Year', 'Journal/Book']).rename(columns = pm_cols)\n",
    "rev_pm_ta = pd.read_csv('./reviews/ta.csv', encoding = 'utf-8', usecols = ['PMID', 'DOI', 'Authors', 'Title', 'Publication Year', 'Journal/Book']).rename(columns = pm_cols)\n",
    "rev_pubmed = pd.concat([rev_pm_bptb, rev_pm_ht, rev_pm_qt, rev_pm_plt, rev_pm_at, rev_pm_ta])\n",
    "\n",
    "rev_pubmed['first_author'] = rev_pubmed['authors'].str.replace(r'[,.;]','', regex = True).str.split().str[0]\n",
    "rev_pubmed['short_title'] = rev_pubmed['title'].str.replace(r'[\\[\\]\\s,.;-]','',regex = True).str.lower()\n",
    "rev_pubmed['key'] = rev_pubmed['first_author'] + '+' + rev_pubmed['short_title'] + '+' + rev_pubmed['year'].astype(str)\n",
    "rev_pubmed.to_csv('./reviews/reviews.csv', encoding = 'utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3236bc97-0ef3-4bd4-be40-114e5f4070d5",
   "metadata": {},
   "source": [
    "**Step 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70fe71b9-b278-4c55-b40d-76219e161164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter file name:  doi_nulls\n",
      "Enter the columns for which to deduplicate based on:  key\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mermaid as md\n",
    "from mermaid.graph import Graph\n",
    "\n",
    "def deduplicate(df, cols):\n",
    "    input_file_name = input('Enter file name: ') + '.csv'\n",
    "    df = pd.read_csv(input_file_name) # A (records)\n",
    "    cols_input = input('Enter the columns for which to deduplicate based on: ')\n",
    "    cols = [c.strip() for c in cols_input.split(',')]\n",
    "    output_file_name = './' + '_'.join(cols) + '_deduplicated.csv'\n",
    "    prisma_file_name = output_file_name.replace('.csv', '.mmd')\n",
    "\n",
    "    nulls_mask = df[cols].isnull().any(axis=1)\n",
    "    df_nulls = df[nulls_mask] # B\n",
    "    df_non_nulls = df[~nulls_mask] # C\n",
    "    \n",
    "    duplicates_mask = df_non_nulls.duplicated(subset = cols, keep = False)\n",
    "    df_non_duplicates = df_non_nulls[~duplicates_mask] # D\n",
    "    df_duplicates = df_non_nulls[duplicates_mask] # E\n",
    "    df_kept = df_duplicates.drop_duplicates(subset = cols, keep = 'first')\n",
    "    df_removed = df_duplicates[~df_duplicates.index.isin(df_kept.index)]\n",
    "    df_unique = df_non_nulls.drop_duplicates(subset = cols, keep = 'first') # df of unique\n",
    "    df_deduplicated = pd.concat([df_non_duplicates, df_kept], ignore_index=True) # df of unique + df of non-duplicates\n",
    "\n",
    "    results = {\n",
    "        \"records\": len(df),\n",
    "        \"nulls\": len(df_nulls),\n",
    "        \"non_nulls\": len(df_non_nulls),\n",
    "        \"non_duplicates\": len(df_non_duplicates),\n",
    "        \"duplicates\": len(df_duplicates),\n",
    "        \"removed\": len(df_removed),\n",
    "        \"kept\": len(df_kept),\n",
    "        \"unique\": len(df_unique),\n",
    "        \"deduplicated\": len(df_deduplicated)\n",
    "    }\n",
    "    \n",
    "    df_nulls.to_csv(output_file_name.replace('deduplicated','nulls'), index = False)\n",
    "    df_deduplicated.to_csv(output_file_name, index = False)\n",
    "    df_removed.to_csv(output_file_name.replace('.csv', '_removed.csv'), index = False)\n",
    "\n",
    "\n",
    "    return results, df_nulls, df_deduplicated, df_kept, df_removed, output_file_name, prisma_file_name\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results, df_nulls, df_deduplicated, df_kept, df_removed, output_file_name, prisma_file_name = deduplicate(df=None, cols=None)\n",
    "    \n",
    "    graph_text = f\"\"\"---\n",
    "config:\n",
    "  theme: neutral\n",
    "  curve: stepBefore\n",
    "---\n",
    "graph TD;\n",
    "A[\"**records** (*n* = {results['records']})\"];\n",
    "B[\"null (*n* = {results['nulls']})\"];\n",
    "C[\"non-null (*n* = {results['non_nulls']})\"];\n",
    "D[\"non-duplicates (*n* = {results['non_duplicates']})\"];\n",
    "E[\"duplicates (*n* = {results['duplicates']})\"];\n",
    "F[\"duplicates kept (*n* = {results['kept']})\"];\n",
    "G[\"duplicates removed (*n* = {results['removed']})\"];\n",
    "H[\"unique (*n* = {results['unique']})\"];\n",
    "I[\"deduplicated (*n* = {results['deduplicated']})\"];\n",
    "\n",
    "A --> B & C;\n",
    "C --> D & E;\n",
    "E --> F & G;\n",
    "D & F --> H\"\"\"\n",
    "    \n",
    "\n",
    "    with open(prisma_file_name, \"w\") as f:\n",
    "        f.write(graph_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc182a99-ef79-42f0-a6f9-5980b0450a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 247 entries, 0 to 13\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0    247 non-null    object\n",
      " 1   id            247 non-null    object\n",
      " 2   title         247 non-null    object\n",
      " 3   authors       247 non-null    object\n",
      " 4   journal       247 non-null    object\n",
      " 5   year          247 non-null    object\n",
      " 6   doi           233 non-null    object\n",
      " 7   first_author  247 non-null    object\n",
      " 8   short_title   247 non-null    object\n",
      " 9   key           247 non-null    object\n",
      "dtypes: object(10)\n",
      "memory usage: 21.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "a = pd.read_csv('./doi_deduplicated.csv', encoding = 'utf-8')\n",
    "b = pd.read_csv('./key_nulls.csv', encoding = 'utf-8')\n",
    "c = pd.read_csv('./key_deduplicated.csv', encoding = 'utf-8')\n",
    "\n",
    "screening = pd.concat([a, b, c])\n",
    "screening.to_csv('./screening (2).csv', encoding = 'utf-8', index = False)\n",
    "screening.head(10)\n",
    "screening.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d592f2-dc88-45fc-8878-ff3d1e5546b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import statsmodels\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "\n",
    "analysis = TTestIndPower()\n",
    "\n",
    "effect_size = float(input('Input the effect size: '))\n",
    "alpha = float(input('Input the alpha value: '))\n",
    "power = float(input('Input the power: '))\n",
    "ratio = float(input('Input the sample size ratio: '))\n",
    "\n",
    "sample_size = analysis.solve_power(\n",
    "    effect_size = effect_size, \n",
    "    alpha = alpha, \n",
    "    power = power, \n",
    "    ratio = ratio)\n",
    "\n",
    "print(f'Minimum sample size needed for experiment: {math.ceil(sample_size)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
